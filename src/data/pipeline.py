"""Pre-processing helpers for Sprint 1 US-03 (feature engineering + split).

The focus is to keep the logic simple, explainable in class, and reusable from
notebooks or scripts. Nothing here is meant to be overly optimizedâ€”just clear.
"""
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from .load_data import load_raw_data

# ---------------------------------------------------------------------------
# Paths
ROOT_DIR = Path(__file__).resolve().parents[2]
PROCESSED_FILE = ROOT_DIR / "data" / "processed" / "processed_data.csv"


@dataclass(slots=True)
class PipelineConfig:
    """Configuration values for the preprocessing pipeline."""

    lag_months: Tuple[int, ...] = (1, 3, 12)
    target_horizon: int = 1  # predict one month ahead
    train_ratio: float = 0.8  # keep last 20% of rows for testing
    feature_columns: Tuple[str, ...] = (
        "SP500",
        "Real Price",
        "PE10",
        "Dividend",
        "Earnings",
        "Consumer Price Index",
        "Long Interest Rate",
    )


@dataclass(slots=True)
class PipelineArtifacts:
    """Container for outputs generated by ``run_pipeline``."""

    processed_df: pd.DataFrame
    feature_columns: List[str]
    target_columns: List[str]
    split_date: pd.Timestamp
    scaler: StandardScaler


# ---------------------------------------------------------------------------
# Feature engineering helpers

def _add_return_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["return_1m"] = df["SP500"].pct_change(1)
    df["return_3m"] = df["SP500"].pct_change(3)
    df["return_12m"] = df["SP500"].pct_change(12)
    return df


def _add_lag_features(df: pd.DataFrame, cols: Iterable[str], lags: Iterable[int]) -> pd.DataFrame:
    df = df.copy()
    for col in cols:
        for lag in lags:
            df[f"{col}_lag_{lag}"] = df[col].shift(lag)
    return df


def _add_targets(df: pd.DataFrame, horizon: int) -> pd.DataFrame:
    df = df.copy()
    future_price = df["SP500"].shift(-horizon)
    df["target_price_next"] = future_price
    df["target_direction"] = (future_price > df["SP500"]).astype(int)
    return df


def engineer_features(df: pd.DataFrame, config: PipelineConfig) -> pd.DataFrame:
    """Create the lag/return features plus future targets."""
    engineered = _add_return_features(df)
    engineered = _add_lag_features(engineered, config.feature_columns, config.lag_months)
    engineered = _add_targets(engineered, config.target_horizon)
    engineered = engineered.dropna()
    return engineered


# ---------------------------------------------------------------------------
# Scaling + split helpers

def _get_split_index(df: pd.DataFrame, ratio: float) -> int:
    split_idx = max(int(len(df) * ratio), 1)
    return min(split_idx, len(df) - 1)  # ensure at least one row for test


def scale_features(df: pd.DataFrame, target_cols: Iterable[str], train_idx: int) -> Tuple[pd.DataFrame, StandardScaler]:
    df_scaled = df.copy()
    feature_cols = [c for c in df.columns if c not in target_cols]

    scaler = StandardScaler()
    scaler.fit(df.iloc[:train_idx][feature_cols])
    df_scaled[feature_cols] = scaler.transform(df[feature_cols])
    return df_scaled, scaler


# ---------------------------------------------------------------------------
# Public pipeline API

def run_pipeline(config: PipelineConfig | None = None) -> PipelineArtifacts:
    """Execute the preprocessing pipeline and save ``processed_data.csv``."""
    config = config or PipelineConfig()
    raw_df = load_raw_data()
    engineered = engineer_features(raw_df, config)

    split_idx = _get_split_index(engineered, config.train_ratio)
    split_date = engineered.index[split_idx]
    target_cols = ["target_price_next", "target_direction"]

    processed_df, scaler = scale_features(engineered, target_cols, split_idx)
    processed_df.to_csv(PROCESSED_FILE, index=True)

    return PipelineArtifacts(
        processed_df=processed_df,
        feature_columns=[c for c in processed_df.columns if c not in target_cols],
        target_columns=target_cols,
        split_date=split_date,
        scaler=scaler,
    )


def summarize_artifacts(artifacts: PipelineArtifacts) -> Dict[str, str]:
    """Return strings that make it easy to print progress info."""
    df = artifacts.processed_df
    summary = {
        "rows": f"Processed rows: {len(df):,}",
        "feature_count": f"Feature columns: {len(artifacts.feature_columns)}",
        "targets": f"Targets: {', '.join(artifacts.target_columns)}",
        "split": f"Train/Test split date: {artifacts.split_date.date()}",
        "output": f"Saved CSV: {PROCESSED_FILE.relative_to(ROOT_DIR)}",
    }
    return summary


if __name__ == "__main__":
    artifacts = run_pipeline()
    for line in summarize_artifacts(artifacts).values():
        print(line)
